{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "from ffn.inference import storage\n",
    "from ffn.inference.segmentation import clear_dust, make_labels_contiguous, clean_up\n",
    "\n",
    "#from cc3d import connected_components\n",
    "\n",
    "\n",
    "import neuroglancer\n",
    "\n",
    "\n",
    "import cloudvolume\n",
    "\n",
    "def hdf5_to_cloudvolume():\n",
    "  pass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT \n",
    "Probably the only part of the script you need to change anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW data information\n",
    "raw_data = 'training_data.h5'\n",
    "raw_data_dset = 'image' #usually don't change\n",
    "\n",
    "##FFN segmentation information\n",
    "seg_folder = './results/inference_01/'\n",
    "seg_seed = (0,0,0)\n",
    "\n",
    "#RESOLUTION information\n",
    "xy_res = 1200\n",
    "z_res = 1200\n",
    "\n",
    "#PROCESSING information\n",
    "cleanup = 0 #0 for nothing , 1 for SS-RAF, 2 for michal, 3 for michal+raf\n",
    "min_particle = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESSING PART\n",
    "Please don't touch!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data info:\n",
      "(128, 128, 128) uint8\n"
     ]
    }
   ],
   "source": [
    "#don't touch this lines\n",
    "h5file = h5py.File(raw_data, 'r+')\n",
    "image = h5file[raw_data_dset]\n",
    "print('Raw data info:')\n",
    "print(image.shape, image.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label data info:\n",
      "(128, 128, 128) uint64\n"
     ]
    }
   ],
   "source": [
    "#don't touch this lines\n",
    "seg, _ = storage.load_segmentation(seg_folder,seg_seed,allow_cpoint=True)\n",
    "print('Label data info:')\n",
    "print(seg.shape, seg.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset info:\n",
      "(128, 128, 128) uint64\n",
      "Number of features:\n",
      "67\n",
      "Cleanup dataset info:\n",
      "(128, 128, 128) uint64\n",
      "Number of features:\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#don't change\n",
    "\n",
    "seg_cleanup = np.zeros(seg.shape)\n",
    "seg_cleanup = seg[:,:,:]\n",
    "\n",
    "\n",
    "if(cleanup==0):\n",
    "    pass\n",
    "\n",
    "elif(cleanup==1):\n",
    "    labels_in = np.zeros(seg.shape,dtype='uint8')\n",
    "    labels_in[seg!=0] = 1\n",
    "    labels_out = connected_components(labels_in)\n",
    "    seg_cleanup = clear_dust(labels_out,min_particle)\n",
    "    \n",
    "elif(cleanup==2):    \n",
    "    clean_up(seg_cleanup, min_size=min_particle)\n",
    "    \n",
    "elif(cleanup==3):\n",
    "    clean_up(seg_cleanup, min_size=min_particle)\n",
    "    \n",
    "    labels_in = np.zeros(seg_cleanup.shape,dtype='uint8')\n",
    "    labels_in[seg_cleanup!=0] = 1\n",
    "    seg_cleanup = connected_components(labels_in)\n",
    "    \n",
    "else:\n",
    "    print(\"put a valid cleanup option!!\")\n",
    "\n",
    "ids  = np.unique(seg, return_counts=1)\n",
    "ids0 = np.unique(seg_cleanup,return_counts=1)\n",
    "\n",
    "print ('Original dataset info:')\n",
    "print(seg.shape,seg.dtype)\n",
    "print ('Number of features:')\n",
    "print (len(ids[0]))\n",
    "\n",
    "print ('Cleanup dataset info:')\n",
    "print(seg_cleanup.shape,seg_cleanup.dtype)\n",
    "print ('Number of features:')\n",
    "print (len(ids0[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "Always execute viewer.close() after you leave otherwise you may leave hell-spawns on your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:45733/v/ab9f1dfa633f2b4408897b657e39d36eed075c86/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 713, in _handle_events\n",
      "    self._handle_write()\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 1063, in _handle_write\n",
      "    self._write_buffer.advance(num_bytes)\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 184, in advance\n",
      "    assert 0 < size <= self._size\n",
      "AssertionError\n",
      "ERROR:asyncio:Exception in callback None()\n",
      "handle: <Handle cancelled>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n",
      "    handler_func(fileobj, events)\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/stack_context.py\", line 300, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 713, in _handle_events\n",
      "    self._handle_write()\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 1063, in _handle_write\n",
      "    self._write_buffer.advance(num_bytes)\n",
      "  File \"/home/rlab/anaconda3/envs/klab/lib/python3.7/site-packages/tornado/iostream.py\", line 184, in advance\n",
      "    assert 0 < size <= self._size\n",
      "AssertionError\n"
     ]
    }
   ],
   "source": [
    "voxel=(z_res,xy_res,xy_res)\n",
    "viewer = neuroglancer.Viewer()\n",
    "with viewer.txn() as s:\n",
    "  s.layers['image'] = neuroglancer.ImageLayer(\n",
    "    source=neuroglancer.LocalVolume(data=image, voxel_size=voxel, volume_type='image'))\n",
    "  s.layers['segmentation_clean'] = neuroglancer.SegmentationLayer(\n",
    "    source=neuroglancer.LocalVolume(data=seg_cleanup, voxel_size=voxel, volume_type='segmentation'),segments=ids0[0])\n",
    "  s.layers['segmentation_original'] = neuroglancer.SegmentationLayer(\n",
    "    source=neuroglancer.LocalVolume(data=seg, voxel_size=voxel, volume_type='segmentation'),segments=ids[0])\n",
    "print(viewer.get_viewer_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del viewer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiff stack save!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dxchange.write_tiff_stack(image,'./stacks/results/cube_', dtype='uint8', overwrite=True)\n",
    "# dxchange.write_tiff_stack(labels_out,'./stacks/results/seg_', dtype='uint64', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WELCOME TO DISNEYLAND\n",
    "## Where dreams come true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rlab/Documents/workspace/knossos_utils/knossos_utils/knossosdataset.py:1911: UserWarning: You are using implicit channel selection. This possibility will soon be removed. Please call set_channel() before reading or writing data using KnossosDataset.\n",
      "  warnings.warn('You are using implicit channel selection. This possibility will soon be removed.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box_offset: [0. 0. 0.]\n",
      "box_size: [128 128 128]\n",
      "start_cube: [0 0 0]\n",
      "end_cube: [1 1 1]\n",
      "box_offset: [0. 0. 0.]\n",
      "box_size: [64 64 64]\n",
      "start_cube: [0 0 0]\n",
      "end_cube: [1 1 1]\n",
      "box_offset: [0. 0. 0.]\n",
      "box_size: [32 32 32]\n",
      "start_cube: [0 0 0]\n",
      "end_cube: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from knossos_utils import knossosdataset\n",
    "#magic happens here.. Magic happens everywhere.. \n",
    "\n",
    "knossos_location = './kcube/'\n",
    "knossos_conf =  'knossos.conf'\n",
    "knossos_new_anno = 'inference01'\n",
    "\n",
    "\n",
    "\n",
    "#don't touch bellow\n",
    "kd = knossosdataset.KnossosDataset()\n",
    "kd.initialize_from_knossos_path(knossos_location+knossos_conf)\n",
    "\n",
    "box_offset = [0,0,0] #don't touch, for future use\n",
    "\n",
    "kd.from_matrix_to_cubes(offset=box_offset, data=seg, as_raw=False, kzip_path=os.path.join(knossos_location,knossos_new_anno))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
