{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "Updated existing app\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "(\"Python search path includes: ['/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/tensorflow/tf1.13/', \"\n",
      " \"'/lus/theta-fs0/projects/connectomics_aesp/keceli/pip_ffn/', \"\n",
      " \"'/gpfs/mira-home/keceli/ffn/keceli_ffn/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/', \"\n",
      " \"'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/', \"\n",
      " \"'/opt/anaconda3/lib/python36.zip', '/opt/anaconda3/lib/python3.6', \"\n",
      " \"'/opt/anaconda3/lib/python3.6/lib-dynload', '', \"\n",
      " \"'/opt/anaconda3/lib/python3.6/site-packages', \"\n",
      " \"'/opt/anaconda3/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg', \"\n",
      " \"'/opt/anaconda3/lib/python3.6/site-packages/IPython/extensions', \"\n",
      " \"'/gpfs/mira-home/rvescovi/.ipython', '/home/rvescovi/.balsam']\")\n",
      "/lus/theta-fs0/projects/connectomics_aesp/balsam_database/\n"
     ]
    }
   ],
   "source": [
    "# The magic commands below allow reflecting the changes in an imported module without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# We need to add balsam and the modules it depends on to the Python search paths. \n",
    "import sys\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/')\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/')\n",
    "\n",
    "import matplotlib.pyplot\n",
    "#sys.path.insert(0,'/lus/theta-fs0/projects/connectomics_aesp/software/neuro_env_36/lib/python3.6/site-packages/')\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(f'Python search path includes: {sys.path}')\n",
    "\n",
    "# We also need postgresql to be in the path\n",
    "import os\n",
    "os.environ['PATH'] ='/soft/datascience/Balsam/0.3.5.1/env/bin/:' + os.environ['PATH']\n",
    "os.environ['PATH'] +=':/soft/datascience/PostgreSQL/9.6.12/bin/'\n",
    "\n",
    "try:\n",
    "    import balsam\n",
    "except:\n",
    "    print('Cannot find balsam, make sure balsam is installed or it is available in Python search paths')    \n",
    "\n",
    "    # We also need to activate Balsam database by setting the BALSAM_DB_PATH environment variable. \n",
    "# This is equivalent to `source balsamactivate jupyter_test` \n",
    "os.environ[\"BALSAM_DB_PATH\"]='/lus/theta-fs0/projects/connectomics_aesp/balsam_database/'\n",
    "\n",
    "\n",
    "from balsam_helper import *\n",
    "import hpn_balsam_apps\n",
    "\n",
    "print(os.environ[\"BALSAM_DB_PATH\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 PREAMBLE AND JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'HappyNeurons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1f1532143c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##Fix import from HPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mHappyNeurons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrakem2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'HappyNeurons'"
     ]
    }
   ],
   "source": [
    "\n",
    "##Fix import from HPN\n",
    "from HappyNeurons import trakem2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "RAW_INPUT=./trakem2_HL00732\n",
    "PROCESS_FOLDER=./trakem2_HL00732_process_3\n",
    "MIN=1024\n",
    "MAX=2048\n",
    "\n",
    "##add pre align tiles files\n",
    "#python $PRE_TILES $RAW_INPUT $PROCESS_FOLDER/align_raw.txt \n",
    "\n",
    "def sem_montage_job(workflow_name, folder):\n",
    "    montage_args = ''\n",
    "    montage_args += f' $PROCESS_FOLDER/align_raw.txt '\n",
    "    montage_args += f' $PROCESS_FOLDER '\n",
    "    montage_args += f' --min $MIN '\n",
    "    montage_args += f' --max $MAX '\n",
    "    montage_args += f' --fiji $FIJI '\n",
    "    print(montage_args)\n",
    "\n",
    "    add_job(name=f'montage',\n",
    "        workflow=workflow_name,\n",
    "        data={folder}\n",
    "        application='trakem2_montage',\n",
    "        args=montage_args,\n",
    "        ranks_per_node=1,\n",
    "        environ_vars='OMP_NUM_THREADS=32')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# python $PRE_STACK $PROCESS_FOLDER/output $PROCESS_FOLDER/align_new.txt #input #output\n",
    "\n",
    "#SERIAL SINGLE NODE OPERATION!!!\n",
    "# $SUBMIT_THETA python $ALIGN $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --fiji $FIJI\n",
    "\n",
    "#$SUBMIT_THETA python $EXPORT $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --range $RANGE --fiji $FIJI\n",
    "#sleep 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLOOD FILL NETWORK PREAMBLE AND JOBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/gpfs/mira-home/keceli/ffn/keceli_ffn/')\n",
    "sys.path.insert(0,'/lus/theta-fs0/projects/connectomics_aesp/keceli/pip_ffn/')\n",
    "sys.path.insert(0,'/soft/datascience/tensorflow/tf1.13/')\n",
    "\n",
    "from ffn.utils import bounding_box\n",
    "from ffn.utils import geom_utils\n",
    "\n",
    "def create_inference_config(pars, file_name):\n",
    "    request_par = ('''image {\n",
    "                  hdf5: \"%s:%s\"\n",
    "                }\n",
    "                image_mean: %s\n",
    "                image_stddev: %s\n",
    "                checkpoint_interval: 1800\n",
    "                seed_policy: \"PolicyPeaks\"\n",
    "                model_checkpoint_path: \"%s\"\n",
    "                model_name: \"convstack_3d.ConvStack3DFFNModel\"\n",
    "                model_args: \"{\\\\\"depth\\\\\": 2, \\\\\"fov_size\\\\\": [5,5,5], \\\\\"deltas\\\\\": [1, 1, 1]}\"\n",
    "                segmentation_output_dir: \"%s\"\n",
    "                inference_options {\n",
    "                  init_activation: 0.9\n",
    "                  pad_value: 0.05\n",
    "                  move_threshold: 0.1\n",
    "                  min_boundary_dist { x: 1 y: 1 z: 1}\n",
    "                  segment_threshold: 0.08\n",
    "                  min_segment_size: 10\n",
    "                }''') % pars\n",
    "    print (request_par)\n",
    "\n",
    "def divide_bounding_box(bbox, subvolume_size, overlap):\n",
    "    \"\"\"\n",
    "    Returns a list of bounding boxes that divides the given bounding box into subvolumes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    bbox: BoundingBox object,\n",
    "    subvolume_size: list or tuple\n",
    "    overlap: list or tuple\n",
    "    \"\"\"\n",
    "    start = geom_utils.ToNumpy3Vector(bbox.start)\n",
    "    size = geom_utils.ToNumpy3Vector(bbox.size)\n",
    "    bbox = bounding_box.BoundingBox(start, size)\n",
    "    calc = bounding_box.OrderlyOverlappingCalculator(outer_box=bbox, \n",
    "                                                    sub_box_size=subvolume_size, \n",
    "                                                    overlap=overlap, \n",
    "                                                    include_small_sub_boxes=True,\n",
    "                                                    back_shift_small_sub_boxes=False)\n",
    "    return [bb for bb in calc.generate_sub_boxes()]\n",
    "\n",
    "def check_balsam_jobs(bbox, config_file):\n",
    "    for i,box in enumerate(boxes):\n",
    "        start = box.start\n",
    "        size  = box.size\n",
    "        print(f\" --bounding_box 'start {{ x:{start[0]} y:{start[1]} z:{start[2]} }} size {{ x:{size[0]} y:{size[1]} z:{size[2]} }}' \")\n",
    "    print(f\" --inference_request=\\\"$(cat \"+config_file+\")\\\" \")\n",
    "\n",
    "\n",
    "def generate_balsam_inference_jobs(bbox_list, config_file, workflow_name='ffn_sub_inference'):\n",
    "    for i,box in enumerate(bbox_list):\n",
    "        start = box.start\n",
    "        size  = box.size\n",
    "        inference_args  = f\" --inference_request=\\\"$(cat \"+config_file+\")\\\" \"\n",
    "        inference_args += f\" --bounding_box 'start {{ x:{start[0]} y:{start[1]} z:{start[2]} }} size {{ x:{size[0]} y:{size[1]} z:{size[2]} }}' \"\n",
    "        add_job(name=f'sub_inference_{i}',\n",
    "                workflow=workflow_name,\n",
    "                application='inference',\n",
    "                args=inference_args,\n",
    "                ranks_per_node=1,\n",
    "                environ_vars='OMP_NUM_THREADS=32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --train_coords /lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file  --data_volumes valdation1:/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5:raw  --label_volumes valdation1:/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5:stack  --model_name convstack_3d.ConvStack3DFFNModel  --model_args \"{\\\"depth\\\": 12, \\\"fov_size\\\": [33, 33, 33], \\\"deltas\\\": [8, 8, 8]}\" --image_mean 128 --image_stddev 33  --max_steps 400 --summary_rate_secs 60  --batch_size 1  --optimizer adam  --num_intra_threads 64 --num_inter_threads 1  --train_dir train_b1_oadam_191001210138 \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_{TIMESTAMP}'\n",
    "\n",
    "\n",
    "##make all of bellow into a single function that receives all of the above!!\n",
    "\n",
    "myargs = ''\n",
    "myargs += f' --train_coords {TFRECORDFILE} '\n",
    "myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "myargs += ' --max_steps 400 --summary_rate_secs 60 ' \n",
    "myargs += f' --batch_size {BATCHSIZE} '\n",
    "myargs += f' --optimizer {OPTIMIZER} '\n",
    "myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "myargs += f' --train_dir {TRAINDIR} '\n",
    "print(myargs)\n",
    "\n",
    "add_job(name='test_train',workflow='ffn_training',application='trainer',args=myargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Inference Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image {\n",
      "                  hdf5: \"input_file_parallel.h5:image\"\n",
      "                }\n",
      "                image_mean: 100\n",
      "                image_stddev: 30\n",
      "                checkpoint_interval: 1800\n",
      "                seed_policy: \"PolicyPeaks\"\n",
      "                model_checkpoint_path: \"model_vessel/model.ckpt-1680104\"\n",
      "                model_name: \"convstack_3d.ConvStack3DFFNModel\"\n",
      "                model_args: \"{\\\"depth\\\": 2, \\\"fov_size\\\": [5,5,5], \\\"deltas\\\": [1, 1, 1]}\"\n",
      "                segmentation_output_dir: \"results_test_parallel6\"\n",
      "                inference_options {\n",
      "                  init_activation: 0.9\n",
      "                  pad_value: 0.05\n",
      "                  move_threshold: 0.1\n",
      "                  min_boundary_dist { x: 1 y: 1 z: 1}\n",
      "                  segment_threshold: 0.08\n",
      "                  min_segment_size: 10\n",
      "                }\n",
      "Number of subvolumes: 125\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Add inference jobs for each subvolume\n",
    "\n",
    "\n",
    "INPUT_FILE = 'input_file_parallel.h5'\n",
    "INPUT_DSET = 'image'\n",
    "OUTPUT_FILE = 'output_file_parallel.h5'\n",
    "OUTPUT_DSET = 'image'\n",
    "OUTPUT_PATH = 'results_test_parallel6'\n",
    "MEAN = 100\n",
    "STD = 30\n",
    "MODEL_PATH = 'model_vessel/model.ckpt-1680104'\n",
    "DEPTH = 2\n",
    "START = (0,0,0)\n",
    "SIZE = (1000,1000,1000)\n",
    "CHUNK_SIZE = (256,256,256)\n",
    "OVERLAP = (16,16,16)\n",
    "\n",
    "\n",
    "##cleanup all of the bellow to receive in a single function all of the above.\n",
    "\n",
    "bbox = bounding_box.BoundingBox(start=START,size=SIZE)\n",
    "pars =  (INPUT_FILE,INPUT_DSET, MEAN, STD, MODEL_PATH, OUTPUT_PATH)\n",
    "\n",
    "test = create_inference_config(pars, 'test')\n",
    "config_file = '/lus/theta-fs0/projects/connectomics_aesp/ravescovi/ffn_vessels_overlay_sean_anno/wholebrain.pbtxt'\n",
    "\n",
    "boxes = divide_bounding_box(bbox,subvolume_size=CHUNK_SIZE,overlap=OVERLAP)\n",
    "print(f'Number of subvolumes: {len(boxes)}')\n",
    "\n",
    "generate_balsam_inference_jobs(boxes, config_file, workflow_name='inference_8_8_v12')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit OK: Qlaunch {   'command': '/lus/theta-fs0/projects/connectomics_aesp/balsam_database/qsubmit/qlaunch66.sh',\n",
      "    'from_balsam': True,\n",
      "    'id': 66,\n",
      "    'job_mode': 'mpi',\n",
      "    'nodes': 128,\n",
      "    'prescheduled_only': False,\n",
      "    'project': 'SDL_workshop',\n",
      "    'queue': 'training',\n",
      "    'scheduler_id': 375826,\n",
      "    'state': 'submitted',\n",
      "    'wall_minutes': 40,\n",
      "    'wf_filter': 'inference_8_8_v12'}\n"
     ]
    }
   ],
   "source": [
    "# If you see 'Submit OK:', Job submission is succesful.\n",
    "\n",
    "workflow = 'inference_8_8_v12'\n",
    "\n",
    "submit(project='SDL_workshop',\n",
    "       queue='training',\n",
    "       nodes=128,\n",
    "       wall_minutes=40,\n",
    "       wf_filter=workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW ANALISER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7efca5558c88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Balsam metadata\n",
    "from balsam.core.models import utilization_report, throughput_report, process_job_times, BalsamJob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "workflow = \"inference_8_8_v12\"\n",
    "\n",
    "##useful shit\n",
    "query = BalsamJob.objects.filter(workflow=workflow)\n",
    "time_dat = process_job_times(query) #filters into a single workflow\n",
    "[j.runtime_seconds for j in query] # full time per balsam Job\n",
    "\n",
    "times_created, num_created = sorted(time_dat['CREATED']), range(1, len(time_dat[\"CREATED\"])+1)\n",
    "\n",
    "t0 = min(times_created)\n",
    "\n",
    "def mins(t):\n",
    "    return (t-t0).total_seconds() / 60\n",
    "\n",
    "plt.step([mins(t) for t in times_created] ,num_created, 'o', where='post',label='creation')\n",
    "times, num_thru = throughput_report(time_dat)\n",
    "plt.step([mins(t) for t in times], num_thru,  'o', where='post', label='done')\n",
    "\n",
    "times_u, num_util = utilization_report(time_dat)\n",
    "plt.step([mins(t) for t in times_u], num_util, 'o', where='post', label='utilization')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-864e826dab68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/soft/datascience/tensorflow/tf1.13/matplotlib/pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcsetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soft/datascience/tensorflow/tf1.13/matplotlib/colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martist\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/soft/datascience/tensorflow/tf1.13/matplotlib/artist.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m from .transforms import (Bbox, IdentityTransform, Transform, TransformedBbox,\n\u001b[1;32m     13\u001b[0m                          TransformedPatchPath, TransformedPath)\n",
      "\u001b[0;32m/soft/datascience/tensorflow/tf1.13/matplotlib/path.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_to_unmasked_float_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_linear_interpolation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_path'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network MultScale Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiscale training job\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "\n",
    "MAGS = [1,2,4,8,16]\n",
    "\n",
    "for mag in MAGS:\n",
    "    \n",
    "    \n",
    "for mag in MAGS:    \n",
    "    TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_m{mag}_{TIMESTAMP}'\n",
    "    myargs = ''\n",
    "    myargs += f' --train_coords {TFRECORDFILE} '\n",
    "    myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "    myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "    myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "    myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "    myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "    myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "    myargs += f' --batch_size {BATCHSIZE} '\n",
    "    myargs += f' --optimizer {OPTIMIZER} '\n",
    "    myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "    myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "    add_job(name=f'train_mag{mag}',\n",
    "            workflow='ffn_training',\n",
    "            application='trainer',\n",
    "            args=myargs,\n",
    "            ranks_per_node=rpn,\n",
    "            num_nodes=nnode,\n",
    "            environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Space Bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a training job to the database\n",
    "import time\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "for rpn in [1,4,16]:\n",
    "    for nnode in [1,4,16,64]:\n",
    "        TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_n{nnode}_r{rpn}_{TIMESTAMP}'\n",
    "        myargs = ''\n",
    "        myargs += f' --train_coords {TFRECORDFILE} '\n",
    "        myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "        myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "        myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "        myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "        myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "        myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "        myargs += f' --batch_size {BATCHSIZE} '\n",
    "        myargs += f' --optimizer {OPTIMIZER} '\n",
    "        myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "        myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "        add_job(name=f'train_n{nnode}_r{rpn}',\n",
    "                workflow='ffn_training',\n",
    "                application='trainer',\n",
    "                args=myargs,\n",
    "                ranks_per_node=rpn,\n",
    "                num_nodes=nnode,\n",
    "                environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit OK: Qlaunch {   'command': '/lus/theta-fs0/projects/connectomics_aesp/balsam_database/qsubmit/qlaunch51.sh',\n",
      "    'from_balsam': True,\n",
      "    'id': 51,\n",
      "    'job_mode': 'mpi',\n",
      "    'nodes': 128,\n",
      "    'prescheduled_only': False,\n",
      "    'project': 'SDL_workshop',\n",
      "    'queue': 'training',\n",
      "    'scheduler_id': 374843,\n",
      "    'state': 'submitted',\n",
      "    'wall_minutes': 40,\n",
      "    'wf_filter': 'inference_8_8_v8'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# submit(project='connectomics_aesp',\n",
    "#        queue='default',\n",
    "#        job_mode='serial',\n",
    "#        nodes=128,\n",
    "#        wall_minutes=180,\n",
    "#        wf_filter='inference_8_8')\n",
    "\n",
    "# submit(project='connectomics_aesp',\n",
    "#        queue='debug-flat-quad',\n",
    "#        nodes=3,\n",
    "#        wall_minutes=59,\n",
    "#        wf_filter='ffn_sub_inference')\n",
    "\n",
    "# submit(project='connectomics_aesp',\n",
    "#        queue='default',\n",
    "#        nodes=256,\n",
    "#        wall_minutes=359,\n",
    "#        wf_filter='ffn_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMTilePreprocessor(object):\n",
    "    \"\"\"Set up EM images for alignment.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Path to the input images.\n",
    "    output : str\n",
    "        Output .txt file with\n",
    "    Attributes\n",
    "    ----------\n",
    "    flist : str\n",
    "        List of input image files. Populated by ``EMTilePreprocessor.run``.\n",
    "    MAX_ROW : int\n",
    "    MAX_COL : int\n",
    "        Unused.\n",
    "    TILE_ROW : int\n",
    "    TILE_COL : int\n",
    "        The number of rows and columns in a given tile within the volume.\n",
    "    TILE_MIN : int\n",
    "    TILE_MAX : int\n",
    "        The min and max grayscale values in all tiles.\n",
    "    DTYPE : int\n",
    "        Data type of the images to align.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dir, output='align.txt'):\n",
    "        self.input_dir = input_dir\n",
    "        self.output = os.path.abspath(output)\n",
    "        output_dir = os.path.dirname(self.output)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.flist = None\n",
    "\n",
    "        self.MAX_ROW = 0\n",
    "        self.MAX_COL = 0\n",
    "        self.TILE_ROW = 0\n",
    "        self.TILE_COL = 0\n",
    "        self.TILE_MIN = 0\n",
    "        self.TILE_MAX = 0\n",
    "        self.DTYPE = 0\n",
    "\n",
    "    def test_one_image(self):\n",
    "        \"\"\"Get metadata from an input image.\"\"\"\n",
    "        # Select one image from the set and open it.\n",
    "        f_dummy = glob.glob(os.path.join(self.input_dir, 'S_*/Tile*.tif'))[0]\n",
    "        dummy_data = cv2.imread(f_dummy, flags=cv2.IMREAD_GRAYSCALE)\n",
    "        print(dummy_data.shape)\n",
    "\n",
    "        # Extract shape and grayscale information.\n",
    "        self.TILE_ROW, self.TILE_COL = dummy_data.shape\n",
    "        # This line extracts the min and max grayscale values for the dummy\n",
    "        # data, but not for the entire volume. Should we be extracting for the\n",
    "        # entire volume or setting it to the data type min/max values instead?\n",
    "        self.TILE_MIN, self.TILE_MAX = np.min(dummy_data[:]), np.max(dummy_data[:])\n",
    "        print(self.TILE_ROW, self.TILE_COL, self.TILE_MIN,\n",
    "              self.TILE_MAX, dummy_data.dtype)\n",
    "\n",
    "        # Extract image data type information.\n",
    "        if dummy_data.dtype == np.uint8:\n",
    "            print('8bit')\n",
    "            self.DTYPE = 0\n",
    "        elif dummy_data.dtype == np.uint16:\n",
    "            print('16bit')\n",
    "            self.DTYPE = 1\n",
    "\n",
    "    def prepare_align_txt(self):\n",
    "        \"\"\"Write out the trakem2 input file.\"\"\"\n",
    "        with open(self.output, 'w') as f_out:\n",
    "            for f in self.flist:\n",
    "                tlist = glob.glob(os.path.join(f, 'Tile_*.tif'))\n",
    "\n",
    "                # This seems superfluous--can it be removed?\n",
    "                if len(tlist) == 0:\n",
    "                    continue\n",
    "\n",
    "                # For every image file, get the filename and its coordinates in\n",
    "                # the entire imaged volume. Then, write out the trakem2 command\n",
    "                # to file, with additional metadata.\n",
    "                for t in tlist:\n",
    "                    res = re.search(r'Tile_r([0-9])-c([0-9])_S_([0-9]+)_*', t)\n",
    "                    tile_name = os.path.abspath(t)\n",
    "                    r = res.group(1)\n",
    "                    c = res.group(2)\n",
    "                    z = int(res.group(3))\n",
    "\n",
    "                    command = '{} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\t {} \\n'.format(\n",
    "                      tile_name, c, r, z, self.TILE_COL, self.TILE_ROW, self.TILE_MIN, self.TILE_MAX, self.DTYPE)\n",
    "                    f_out.write(command)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Preprocess image data to run through trakem2.\"\"\"\n",
    "        print(\"Input:\", self.input_dir)\n",
    "        print(\"Output:\", self.output)\n",
    "\n",
    "        # Get the image filenames and sort by their index.\n",
    "        self.flist = glob.glob(os.path.join(self.input_dir, 'S_*'))\n",
    "\n",
    "        def get_index(f):\n",
    "            \"\"\"Retrieve the index from an image filename.\n",
    "            Returns\n",
    "            -------\n",
    "            idx : str\n",
    "                The string index of the image file.\n",
    "            \"\"\"\n",
    "            return re.search(r'([0-9]+)', os.path.basename(f)).group(1)\n",
    "\n",
    "        self.flist.sort(key=get_index)\n",
    "\n",
    "        # Get image metadata.\n",
    "        self.test_one_image()\n",
    "\n",
    "        # Write out the trakem2 input text file.\n",
    "        self.prepare_align_txt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ALIGNTK APPS \n",
    "\n",
    "##TODO jobalize the layers bellow\n",
    "\n",
    "#!/bin/bash\n",
    "#COBALT -t 180\n",
    "#COBALT -n 128\n",
    "#COBALT -q default\n",
    "#COBALT -A connectomics_aesp\n",
    "\n",
    "# NODES=$COBALT_JOBSIZE\n",
    "# PROC_PER_NODE=64\n",
    "# PROC=$((NODES * PROC_PER_NODE))\n",
    "#PROC=777\n",
    "\n",
    "# ALIGNTK_DIR=/projects/connectomics_aesp/software/aligntk-1.0.2/install/bin\n",
    "# IMAGE_DIR=\"../data/images_corr_v2/\"\n",
    "# MASK_DIR=\"../masks_corr_v3/\"\n",
    "# OUTPUT_DIR=\"./outputs_v1\"\n",
    "# GROUP_SIZE=$((PROC-1))\n",
    "# N_IMAGES=`ls $IMAGE_DIR | wc -l`\n",
    "# N_GROUPS=$((N_IMAGES / GROUP_SIZE + 1))\n",
    "# #GROUPS=`seq 0 $((N_GROUPS - 1))`\n",
    "\n",
    "# echo \"Total image count: $N_IMAGES\"\n",
    "# echo \"Number of groups: $N_GROUPS\"\n",
    "\n",
    "# mkdir -p logs\n",
    "# mkdir -p $MASK_DIR\n",
    "# mkdir -p $OUTPUT_DIR/cmaps\n",
    "# mkdir -p $OUTPUT_DIR/logs\n",
    "# mkdir -p $OUTPUT_DIR/amaps\n",
    "# mkdir -p $OUTPUT_DIR/grids\n",
    "# mkdir -p $OUTPUT_DIR/maps\n",
    "# mkdir -p $OUTPUT_DIR/aligned\n",
    "\n",
    "# def make_schedule:\n",
    "\n",
    "#     fprint \"10   1.0  0.1\n",
    "#      9   1.0  0.1\n",
    "#      8   1.0  0.3\n",
    "#      7   1.0  1.0\n",
    "#      7   1.0  2.0\n",
    "#      7   1.0  5.0\n",
    "#      6   1.0  5.0\" > schedule.lst\n",
    "\n",
    "# module load miniconda-3.6/conda-4.5.4\n",
    "# source activate ~/workspace/envs/py36\n",
    "\n",
    "\n",
    "## preprocess\n",
    "#aligntk_preprocess --image_dir $IMAGE_DIR --output_dir . --group_size $GROUP_SIZE\n",
    "#aprun -n 777 -N $PROC_PER_NODE python -m klab_utils.aligntk_gen_mask --image_dir $IMAGE_DIR --mask_dir $MASK_DIR --low 10 --high 240 --kernel 10\n",
    "\n",
    "## find rst\n",
    "#for (( i=0; i<$N_GROUPS; i++ ))\n",
    "#do\n",
    "#  echo \"find_rst: $i\"\n",
    "#  START=`date +\"%s\"`\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/find_rst -pairs pairs$i.lst -tif -images $IMAGE_DIR -mask $MASK_DIR -output $OUTPUT_DIR/cmaps/ -rotation -15-15 -max_res 8192 -scale 0.8-1.2 -tx -30-30 -ty -30-30 -summary $OUTPUT_DIR/cmaps/summary$i.out\n",
    "#\tNOW=`date +\"%s\"`\n",
    "#\techo $(((NOW - START)/60)) minutes\n",
    "#done\n",
    "#\n",
    "## register\n",
    "#for (( i=0; i<$N_GROUPS; i++ ))\n",
    "#do\n",
    "#  echo \"register: $i\"\n",
    "#  START=`date +\"%s\"`\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs$i.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 6.0 -output_level 6 -depth 6 -quality 0.5 -summary $OUTPUT_DIR/maps/summary$i.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "#\tNOW=`date +\"%s\"`\n",
    "#\techo $(((NOW - START)/60)) minutes\n",
    "#done\n",
    "\n",
    "#echo \"register\"\n",
    "#START=`date +\"%s\"`\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 4.0 -output_level 6 -depth 6 -quality 0.3 -summary $OUTPUT_DIR/maps/summary.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "#NOW=`date +\"%s\"`\n",
    "\n",
    "## align\n",
    "#FIXED=`head images.lst -n 1`\n",
    "# echo \"align\"\n",
    "# START=`date +\"%s\"`\n",
    "# aprun -n 8192 -N 64 $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -maps $OUTPUT_DIR/maps/ -masks $MASK_DIR -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -masks $MASK_DIR -maps $OUTPUT_DIR/maps/ -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "# NOW=`date +\"%s\"`\n",
    "# echo $(((NOW - START)/60)) minutes\n",
    "\n",
    "\n",
    "add_app(name='aligntk_apply_map',\n",
    "        executable='python -m klab_utils.aligntk_mpi_apply_map',\n",
    "        description='Distributed FFN training script',\n",
    "        envscript='/lus/theta-fs0/projects/connectomics_aesp/software/macros_theta/theta_balsam_preamble.sh')\n",
    "\n",
    "\n",
    "#START=`date +\"%s\"`\n",
    "##$ALIGNTK_DIR/apply_map -image_list images.lst -images $IMAGE_DIR -maps $OUTPUT_DIR/amaps/ -output $OUTPUT_DIR/aligned/ -memory 150000\n",
    "#aprun -n 777 -N 64 python -m klab_utils.aligntk_mpi_apply_map ./outputs_v1/ --image_dir ../data/images_corr_v2/ --image_lst ./images.lst\n",
    "#\n",
    "#NOW=`date +\"%s\"`\n",
    "#echo $(((NOW - START)/60)) minutes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
