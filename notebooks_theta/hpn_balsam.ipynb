{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/lus/theta-fs0/projects/connectomics_aesp/software/neuro_env_36/bin/python\r\n",
      "python: error while loading shared libraries: libpython3.6m.so.1.0: cannot open shared object file: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# The magic commands below allow reflecting the changes in an imported module without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "!source /lus/theta-fs0/projects/connectomics_aesp/software/neuro_env_36/bin/activate-nomodules && which python && python -m ipykernel install --user --name jhub_neurenev36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add balsam and the modules it depends on to the Python search paths. \n",
    "import sys\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/')\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/')\n",
    "\n",
    "pprint(f'Python search path includes: {sys.path}')\n",
    "\n",
    "# We also need postgresql to be in the path\n",
    "import os\n",
    "os.environ['PATH'] ='/soft/datascience/Balsam/0.3.5.1/env/bin/:' + os.environ['PATH']\n",
    "os.environ['PATH'] +=':/soft/datascience/PostgreSQL/9.6.12/bin/'\n",
    "\n",
    "try:\n",
    "    import balsam\n",
    "except:\n",
    "    print('Cannot find balsam, make sure balsam is installed or it is available in Python search paths')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to activate Balsam database by setting the BALSAM_DB_PATH environment variable. \n",
    "# This is equivalent to `source balsamactivate jupyter_test` \n",
    "os.environ[\"BALSAM_DB_PATH\"]='/lus/theta-fs0/projects/connectomics_aesp/ravescovi/balsam_db/'\n",
    "print(os.environ[\"BALSAM_DB_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Building balsam app/job database based on the HappyNeurons applications.\n",
    "from balsam_helper import *\n",
    "import hpn_balsam_apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 MONTAGE JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = \"trakem2_test_v1\"\n",
    "##\n",
    "RAW_INPUT=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_HL00732/\"\n",
    "PROCESS_FOLDER=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam5/\"\n",
    "TARGET_FOLDER =\"\"\n",
    "MIN=1000\n",
    "MAX=1500\n",
    "##\n",
    "job = hpn_balsam_apps.trakem_montage_job(workflow, RAW_INPUT, PROCESS_FOLDER,target=TARGET_FOLDER, min=MIN, max=MAX, num_nodes=2)\n",
    "print (job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 ALIGN JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 SCRATCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python $PRE_STACK $PROCESS_FOLDER/output $PROCESS_FOLDER/align_new.txt #input #output\n",
    "\n",
    "#SERIAL SINGLE NODE OPERATION!!!\n",
    "# $SUBMIT_THETA python $ALIGN $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --fiji $FIJI\n",
    "\n",
    "#$SUBMIT_THETA python $EXPORT $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --range $RANGE --fiji $FIJI\n",
    "#sleep 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN Compute Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_comp_part_v1'\n",
    "##\n",
    "INPUT_VOLUME = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/data/sample_A_subset_ffn.h5'\n",
    "INPUT_VOLUME_DSET = 'volumes/labels/neuron_ids_8'\n",
    "OUTPUT_VOLUME = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/run1/sample_A_subset_parts.h5'\n",
    "THRESHOLDS = '0.025,0.05,0.075,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9'\n",
    "LOM_RADIUS = '5,5,5'\n",
    "MIN_SIZE = '50'\n",
    "##  \n",
    "job = hpn_balsam_apps.ffn_compute_partitions_job(INPUT_VOLUME, INPUT_VOLUME_DSET, OUTPUT_VOLUME, THRESHOLDS, LOM_RADIUS, MIN_SIZE, workflow,num_nodes=4)\n",
    "print (job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN Build Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_comp_part_v1'\n",
    "##\n",
    "TFRECORDFILE = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/run1/tf_record_file'\n",
    "PARTITION_DATA = OUTPUT_VOLUME\n",
    "SAMPLE = 'Cremi_A_subset'\n",
    "MARGIN = '5,5,5'\n",
    "##\n",
    "job = hpn_balsam_apps.ffn_build_coordinates_job(SAMPLE, PARTITION_DATA, TFRECORDFILE, MARGIN, workflow, num_nodes=4)    \n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_train'\n",
    "##\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_{TIMESTAMP}'\n",
    "DEPTH = 12\n",
    "FOV = \"33, 33, 33\" \n",
    "DELTA = \"8, 8, 8\"\n",
    "##\n",
    "job = hpn_balsam_apps.ffn_train_network_job(TFRECORDFILE, GROUNDTRUTH, GRAYSCALE, BATCHSIZE, OPTIMIZER, TIMESTAMP, TRAINDIR, DEPTH, FOV, DELTA, workflow)  \n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Inference Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow='ffn_parallel_inference'\n",
    "##\n",
    "INPUT_FILE = 'input_file_parallel.h5'\n",
    "INPUT_DSET = 'image'\n",
    "OUTPUT_FILE = 'output_file_parallel.h5'\n",
    "OUTPUT_DSET = 'image'\n",
    "OUTPUT_PATH = 'results_test_parallel6'\n",
    "MEAN = 100\n",
    "STD = 30\n",
    "MODEL_PATH = 'model_vessel/model.ckpt-1680104'\n",
    "DEPTH = 2\n",
    "START = (0,0,0)\n",
    "SIZE = (1000,1000,1000)\n",
    "CHUNK_SIZE = (256,256,256)\n",
    "OVERLAP = (16,16,16)\n",
    "##\n",
    "##cleanup all of the bellow to receive in a single function all of the above.\n",
    "bbox = bounding_box.BoundingBox(start=START,size=SIZE)\n",
    "pars =  (INPUT_FILE,INPUT_DSET, MEAN, STD, MODEL_PATH, OUTPUT_PATH)\n",
    "##\n",
    "test = create_inference_config(pars, 'test')\n",
    "config_file = '/lus/theta-fs0/projects/connectomics_aesp/ravescovi/ffn_vessels_overlay_sean_anno/wholebrain.pbtxt'\n",
    "##\n",
    "boxes = divide_bounding_box(bbox,subvolume_size=CHUNK_SIZE,overlap=OVERLAP)\n",
    "print(f'Number of subvolumes: {len(boxes)}')\n",
    "##\n",
    "jobs = hpn_balsam_apps.generate_balsam_inference_jobs(boxes, config_file, workflow_name='inference_8_8_v13')\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you see 'Submit OK:', Job submission is succesful.\n",
    "\n",
    "workflow = ''\n",
    "\n",
    "submit(project='connectomics_aesp',\n",
    "       queue='debug-flat-quad',\n",
    "       nodes=4,\n",
    "       job_mode='mpi',\n",
    "       wall_minutes=40,\n",
    "       wf_filter=workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW ANALISER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balsam metadata\n",
    "from balsam.core.models import utilization_report, throughput_report, process_job_times, BalsamJob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "workflow = \"trakem2_test_v3\"\n",
    "\n",
    "##useful stuff\n",
    "query = BalsamJob.objects.filter(workflow=workflow)\n",
    "time_dat = process_job_times(query) #filters into a single workflow\n",
    "[j.runtime_seconds for j in query] # full time per balsam Job\n",
    "\n",
    "times_created, num_created = sorted(time_dat['CREATED']), range(1, len(time_dat[\"CREATED\"])+1)\n",
    "\n",
    "t0 = min(times_created)\n",
    "\n",
    "def mins(t):\n",
    "    return (t-t0).total_seconds() / 60\n",
    "\n",
    "plt.step([mins(t) for t in times_created] ,num_created, 'o', where='post',label='creation')\n",
    "times, num_thru = throughput_report(time_dat)\n",
    "plt.step([mins(t) for t in times], num_thru,  'o', where='post', label='done')\n",
    "\n",
    "times_u, num_util = utilization_report(time_dat)\n",
    "plt.step([mins(t) for t in times_u], num_util, 'o', where='post', label='utilization')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network MultScale Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiscale training job\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "\n",
    "MAGS = [1,2,4,8,16]\n",
    "\n",
    "##DOWNSAMPLE INPUT DATA AND ORGANIZE DIRS\n",
    "#for mag in MAGS:\n",
    "    \n",
    "    \n",
    "for mag in MAGS:    \n",
    "    TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_m{mag}_{TIMESTAMP}'\n",
    "    myargs = ''\n",
    "    myargs += f' --train_coords {TFRECORDFILE} '\n",
    "    myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "    myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "    myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "    myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "    myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "    myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "    myargs += f' --batch_size {BATCHSIZE} '\n",
    "    myargs += f' --optimizer {OPTIMIZER} '\n",
    "    myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "    myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "    add_job(name=f'train_mag{mag}',\n",
    "            workflow='ffn_training',\n",
    "            application='trainer',\n",
    "            args=myargs,\n",
    "            ranks_per_node=rpn,\n",
    "            num_nodes=nnode,\n",
    "            environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Space Bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a training job to the database\n",
    "import time\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "for rpn in [1,4,16]:\n",
    "    for nnode in [1,4,16,64]:\n",
    "        TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_n{nnode}_r{rpn}_{TIMESTAMP}'\n",
    "        myargs = ''\n",
    "        myargs += f' --train_coords {TFRECORDFILE} '\n",
    "        myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "        myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "        myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "        myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "        myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "        myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "        myargs += f' --batch_size {BATCHSIZE} '\n",
    "        myargs += f' --optimizer {OPTIMIZER} '\n",
    "        myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "        myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "        add_job(name=f'train_n{nnode}_r{rpn}',\n",
    "                workflow='ffn_training',\n",
    "                application='trainer',\n",
    "                args=myargs,\n",
    "                ranks_per_node=rpn,\n",
    "                num_nodes=nnode,\n",
    "                environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ALIGNTK APPS \n",
    "\n",
    "from glob import glob\n",
    "\n",
    "##TODO jobalize the layers bellow\n",
    "\n",
    "#!/bin/bash\n",
    "#COBALT -t 180\n",
    "#COBALT -n 128\n",
    "#COBALT -q default\n",
    "#COBALT -A connectomics_aesp\n",
    "\n",
    "# NODES=$COBALT_JOBSIZE\n",
    "# PROC_PER_NODE=64\n",
    "# PROC=$((NODES * PROC_PER_NODE))\n",
    "PROC=777\n",
    "\n",
    "ALIGNTK_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/software/aligntk-1.0.2/install/bin\"\n",
    "IMAGE_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_HL00732_process_1/output/\"\n",
    "MASK_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/aligntk_HL00732/out1_mask\"\n",
    "OUTPUT_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/aligntk_HL00732/out1\"\n",
    "GROUP_SIZE=PROC-1\n",
    "N_IMAGES= len(glob(IMAGE_DIR+'*'))\n",
    "N_GROUPS=((N_IMAGES / GROUP_SIZE + 1))\n",
    "\n",
    "\n",
    "def aligntk_create_dirs(OUTPUT_DIR, MASK_DIR):\n",
    "    os.mkdir(OUTPUT_DIR)\n",
    "    os.mkdir(MASK_DIR)\n",
    "    os.mkdir(OUTPUT_DIR+'/cmaps')\n",
    "    os.mkdir(OUTPUT_DIR+'/logs')\n",
    "    os.mkdir(OUTPUT_DIR+'/amaps')\n",
    "    os.mkdir(OUTPUT_DIR+'/grids')\n",
    "    os.mkdir(OUTPUT_DIR+'/maps')\n",
    "    os.mkdir(OUTPUT_DIR+'/aligned')\n",
    "    \n",
    "\n",
    "def make_schedule(OUTPUT_DIR):\n",
    "    \n",
    "    schedule = ['10   1.0  0.1',\n",
    "                '9   1.0  0.1',\n",
    "                '8   1.0  0.3',\n",
    "                '7   1.0  1.0',\n",
    "                '7   1.0  2.0',\n",
    "                '7   1.0  5.0',\n",
    "                '6   1.0  5.0']\n",
    "    \n",
    "    file = open(OUTPUT_DIR+\"/schedule.lst\",\"w+\") \n",
    "    for L in schedule:\n",
    "        file.writelines(L)\n",
    "        file.writelines('\\n')\n",
    "    file.close() \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total image count: \"+str(N_IMAGES))\n",
    "print(\"Number of groups: \"+str(N_GROUPS))\n",
    "\n",
    "#aligntk_create_dirs(OUTPUT_DIR, MASK_DIR)\n",
    "#make_schedule(OUTPUT_DIR)\n",
    "#preprocess_main(IMAGE_DIR, OUTPUT_DIR, 12)\n",
    "\n",
    "from happyneuron.aligntk.preprocess import *\n",
    "\n",
    "def aligntk_preprocess_job(image_dir, mask_dir, low, high, kernel, workflow)\n",
    "\n",
    "\n",
    "  #aprun -n 777 -N $PROC_PER_NODE python -m klab_utils.aligntk_gen_mask --image_dir $IMAGE_DIR --mask_dir $MASK_DIR --low 10 --high 240 --kernel 10\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_app(name='aligntk_findrst',\n",
    "        executable='python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/find_rst.py',\n",
    "        description='AlignTK Apply Map',\n",
    "        envscript=env_preamble)\n",
    "\n",
    "def aligntk_rst_job( aligntk=''):\n",
    "\n",
    "    \n",
    "    for i in np.arange(N_GROUPS):\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/find_rst -pairs pairs$i.lst -tif -images $IMAGE_DIR -mask $MASK_DIR -output $OUTPUT_DIR/cmaps/ -rotation -15-15 -max_res 8192 -scale 0.8-1.2 -tx -30-30 -ty -30-30 -summary $OUTPUT_DIR/cmaps/summary$i.out\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_register_job():\n",
    "#for (( i=0; i<$N_GROUPS; i++ ))\n",
    "\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs$i.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 6.0 -output_level 6 -depth 6 -quality 0.5 -summary $OUTPUT_DIR/maps/summary$i.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "##\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 4.0 -output_level 6 -depth 6 -quality 0.3 -summary $OUTPUT_DIR/maps/summary.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_align_job():\n",
    "#FIXED=`head images.lst -n 1`\n",
    "# aprun -n 8192 -N 64 $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -maps $OUTPUT_DIR/maps/ -masks $MASK_DIR -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -masks $MASK_DIR -maps $OUTPUT_DIR/maps/ -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_applymap_job():\n",
    "##$ALIGNTK_DIR/apply_map -image_list images.lst -images $IMAGE_DIR -maps $OUTPUT_DIR/amaps/ -output $OUTPUT_DIR/aligned/ -memory 150000\n",
    "#aprun -n 777 -N 64 python -m klab_utils.aligntk_mpi_apply_map ./outputs_v1/ --image_dir ../data/images_corr_v2/ --image_lst ./images.lst\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
