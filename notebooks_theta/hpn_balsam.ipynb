{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# The magic commands below allow reflecting the changes in an imported module without restarting the kernel.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#!source /lus/theta-fs0/projects/connectomics_aesp/software/neuro_env_36/bin/activate-nomodules && LD_LIBRARY_PATH=/soft/interpreters/python/3.6/intel/2019.3.075/lib/:$LD_LIBRARY_PATH python -m ipykernel install --user --name jhub_neurenev36 && echo \"success\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add balsam and the modules it depends on to the Python search paths. \n",
    "import sys\n",
    "import os\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot\n",
    "import balsam\n",
    "from balsam.launcher.dag import submit\n",
    "\n",
    "#Note to self..\n",
    "#I don't like this..\n",
    "sys.path.insert(0,'/lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/')\n",
    "##I really don't like this..\n",
    "sys.path.insert(0,'/lus/theta-fs0/projects/connectomics_aesp/software/neuro_env_36/lib/python3.6/site-packages/')\n",
    "##and this one..\n",
    "os.environ['PATH'] +=':/soft/datascience/PostgreSQL/9.6.12/bin/'\n",
    "\n",
    "\n",
    "import happyneuron as hpn\n",
    "from  happyneuron.balsam.rvescovi_paths import *\n",
    "import happyneuron.balsam\n",
    "import happyneuron.balsam.hpn_balsam_apps as apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'happyneuron' has no attribute 'balsam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-69304397071d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# This is equivalent to `source balsamactivate jupyter_test`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdatabase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/lus/theta-fs0/projects/connectomics_aesp/ravescovi/balsam_db/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbalsam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBalsamContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m##there is some circularity here.. the upper cells fails until you run this one but this one fails if you don't run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'happyneuron' has no attribute 'balsam'"
     ]
    }
   ],
   "source": [
    "# We also need to activate Balsam database by setting the BALSAM_DB_PATH environment variable. \n",
    "# This is equivalent to `source balsamactivate jupyter_test` \n",
    "database_path='/lus/theta-fs0/projects/connectomics_aesp/ravescovi/balsam_db/'\n",
    "hpn.balsam.BalsamContext(database_path)\n",
    "\n",
    "##there is some circularity here.. the upper cells fails until you run this one but this one fails if you don't run the \n",
    "##upper one.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/trakem2/mpi_montage.py\n",
      "Created new app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/trakem2/preprocess_stack.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/trakem2/align.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/trakem2/mpi_export.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/gen_mask.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/mpi_apply_map.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/find_rst.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/register.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/ffn/build_coordinates_mpi.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/ffn/compute_partitions_mpi.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/ffn/train_hvd.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/ffn/run_inference.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/io/img_to_cloudvolume.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/mesh/mesh_generator.py\n",
      "Updated existing app\n",
      "python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/io/hdf5_to_cloudvolume.py\n",
      "Updated existing app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<happyneuron.balsam.BalsamContext at 0x7feefe009198>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 MONTAGE JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_HL00732/\n",
      "Output: /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6/align_raw.txt\n",
      "(14000, 10833)\n",
      "14000 10833 0 254 uint8\n",
      "8bit\n",
      "BalsamJob 7460ef01-4a18-44d0-befc-40a96020e05f\n",
      "----------------------------------------------\n",
      "workflow:                       trakem2_test_v4\n",
      "name:                           montage\n",
      "description:                    \n",
      "lock:                           \n",
      "parents:                        []\n",
      "input_files:                    *\n",
      "stage_in_url:                   \n",
      "stage_out_files:                \n",
      "stage_out_url:                  \n",
      "wall_time_minutes:              1\n",
      "num_nodes:                      2\n",
      "coschedule_num_nodes:           0\n",
      "ranks_per_node:                 4\n",
      "cpu_affinity:                   depth\n",
      "threads_per_rank:               1\n",
      "threads_per_core:               1\n",
      "node_packing_count:             1\n",
      "environ_vars:                   OMP_NUM_THREADS=32\n",
      "application:                    trakem2_montage\n",
      "args:                            /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6//align_raw.txt  /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6/  --min 1000  --max 1500  --fiji /projects/connectomics_aesp/software/Fiji.app/ImageJ-linux64 \n",
      "user_workdir:                   \n",
      "wait_for_parents:               True\n",
      "post_error_handler:             False\n",
      "post_timeout_handler:           False\n",
      "auto_timeout_retry:             True\n",
      "state:                          CREATED\n",
      "queued_launch_id:               None\n",
      "data:                           {}\n",
      "  *** Executed command:         python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/trakem2/mpi_montage.py /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6//align_raw.txt /lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6/ --min 1000 --max 1500 --fiji /projects/connectomics_aesp/software/Fiji.app/ImageJ-linux64\n",
      "  *** Working directory:        /lus/theta-fs0/projects/connectomics_aesp/ravescovi/balsam_db/data/trakem2_test_v4/montage_7460ef01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "workflow = \"trakem2_test_v4\"\n",
    "##\n",
    "RAW_INPUT=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_HL00732/\"\n",
    "PROCESS_FOLDER=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_balsam6/\"\n",
    "TARGET_FOLDER =\"\"\n",
    "MIN=1000\n",
    "MAX=1500\n",
    "##\n",
    "job = apps.trakem_montage_job(workflow, RAW_INPUT, PROCESS_FOLDER,target=TARGET_FOLDER, min=MIN, max=MAX, num_nodes=2)\n",
    "print (job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 ALIGN JOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAKEM2 SCRATCH SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python $PRE_STACK $PROCESS_FOLDER/output $PROCESS_FOLDER/align_new.txt #input #output\n",
    "\n",
    "#SERIAL SINGLE NODE OPERATION!!!\n",
    "# $SUBMIT_THETA python $ALIGN $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --fiji $FIJI\n",
    "\n",
    "#$SUBMIT_THETA python $EXPORT $PROCESS_FOLDER/align_new.txt $PROCESS_FOLDER/align1 --range $RANGE --fiji $FIJI\n",
    "#sleep 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN Compute Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_comp_part_v1'\n",
    "##\n",
    "INPUT_VOLUME = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/data/sample_A_subset_ffn.h5'\n",
    "INPUT_VOLUME_DSET = 'volumes/labels/neuron_ids_8'\n",
    "OUTPUT_VOLUME = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/run1/sample_A_subset_parts.h5'\n",
    "THRESHOLDS = '0.025,0.05,0.075,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9'\n",
    "LOM_RADIUS = '5,5,5'\n",
    "MIN_SIZE = '50'\n",
    "##  \n",
    "job = apps.ffn_compute_partitions_job(INPUT_VOLUME, INPUT_VOLUME_DSET, OUTPUT_VOLUME, THRESHOLDS, LOM_RADIUS, MIN_SIZE, workflow,num_nodes=4)\n",
    "print (job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFN Build Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_comp_part_v1'\n",
    "##\n",
    "TFRECORDFILE = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/ffn_preprocess/run1/tf_record_file'\n",
    "PARTITION_DATA = OUTPUT_VOLUME\n",
    "SAMPLE = 'Cremi_A_subset'\n",
    "MARGIN = '5,5,5'\n",
    "##\n",
    "job = apps.ffn_build_coordinates_job(SAMPLE, PARTITION_DATA, TFRECORDFILE, MARGIN, workflow, num_nodes=4)    \n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = 'ffn_train'\n",
    "##\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_{TIMESTAMP}'\n",
    "DEPTH = 12\n",
    "FOV = \"33, 33, 33\" \n",
    "DELTA = \"8, 8, 8\"\n",
    "##\n",
    "job = apps.ffn_train_network_job(TFRECORDFILE, GROUNDTRUTH, GRAYSCALE, BATCHSIZE, OPTIMIZER, TIMESTAMP, TRAINDIR, DEPTH, FOV, DELTA, workflow)  \n",
    "print(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network Inference Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow='ffn_parallel_inference'\n",
    "##\n",
    "INPUT_FILE = 'input_file_parallel.h5'\n",
    "INPUT_DSET = 'image'\n",
    "OUTPUT_FILE = 'output_file_parallel.h5'\n",
    "OUTPUT_DSET = 'image'\n",
    "OUTPUT_PATH = 'results_test_parallel6'\n",
    "MEAN = 100\n",
    "STD = 30\n",
    "MODEL_PATH = 'model_vessel/model.ckpt-1680104'\n",
    "DEPTH = 2\n",
    "START = (0,0,0)\n",
    "SIZE = (1000,1000,1000)\n",
    "CHUNK_SIZE = (256,256,256)\n",
    "OVERLAP = (16,16,16)\n",
    "##\n",
    "##cleanup all of the bellow to receive in a single function all of the above.\n",
    "bbox = bounding_box.BoundingBox(start=START,size=SIZE)\n",
    "pars =  (INPUT_FILE,INPUT_DSET, MEAN, STD, MODEL_PATH, OUTPUT_PATH)\n",
    "##\n",
    "test = create_inference_config(pars, 'test')\n",
    "config_file = '/lus/theta-fs0/projects/connectomics_aesp/ravescovi/ffn_vessels_overlay_sean_anno/wholebrain.pbtxt'\n",
    "##\n",
    "boxes = divide_bounding_box(bbox,subvolume_size=CHUNK_SIZE,overlap=OVERLAP)\n",
    "print(f'Number of subvolumes: {len(boxes)}')\n",
    "##\n",
    "jobs = apps.generate_balsam_inference_jobs(boxes, config_file, workflow_name='inference_8_8_v13')\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit OK: Qlaunch {   'command': '/lus/theta-fs0/projects/connectomics_aesp/ravescovi/balsam_db/qsubmit/qlaunch12.sh',\n",
      "    'from_balsam': True,\n",
      "    'id': 12,\n",
      "    'job_mode': 'mpi',\n",
      "    'nodes': 1,\n",
      "    'prescheduled_only': False,\n",
      "    'project': 'connectomics_aesp',\n",
      "    'queue': 'debug-flat-quad',\n",
      "    'scheduler_id': 386126,\n",
      "    'state': 'submitted',\n",
      "    'wall_minutes': 40,\n",
      "    'wf_filter': ''}\n"
     ]
    }
   ],
   "source": [
    "# If you see 'Submit OK:', Job submission is succesful.\n",
    "\n",
    "workflow = ''\n",
    "\n",
    "submit(project='connectomics_aesp',\n",
    "       queue='debug-flat-quad',\n",
    "       nodes=1,\n",
    "       job_mode='mpi',\n",
    "       wall_minutes=40,\n",
    "       wf_filter=workflow)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BALSAM WORKFLOW ANALISER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Balsam metadata\n",
    "from balsam.core.models import utilization_report, throughput_report, process_job_times, BalsamJob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "workflow = \"trakem2_test_v3\"\n",
    "\n",
    "##useful stuff\n",
    "query = BalsamJob.objects.filter(workflow=workflow)\n",
    "time_dat = process_job_times(query) #filters into a single workflow\n",
    "[j.runtime_seconds for j in query] # full time per balsam Job\n",
    "\n",
    "times_created, num_created = sorted(time_dat['CREATED']), range(1, len(time_dat[\"CREATED\"])+1)\n",
    "\n",
    "t0 = min(times_created)\n",
    "\n",
    "def mins(t):\n",
    "    return (t-t0).total_seconds() / 60\n",
    "\n",
    "plt.step([mins(t) for t in times_created] ,num_created, 'o', where='post',label='creation')\n",
    "times, num_thru = throughput_report(time_dat)\n",
    "plt.step([mins(t) for t in times], num_thru,  'o', where='post', label='done')\n",
    "\n",
    "times_u, num_util = utilization_report(time_dat)\n",
    "plt.step([mins(t) for t in times_u], num_util, 'o', where='post', label='utilization')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Fill Network MultScale Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiscale training job\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "\n",
    "MAGS = [1,2,4,8,16]\n",
    "\n",
    "##DOWNSAMPLE INPUT DATA AND ORGANIZE DIRS\n",
    "#for mag in MAGS:\n",
    "    \n",
    "    \n",
    "for mag in MAGS:    \n",
    "    TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_m{mag}_{TIMESTAMP}'\n",
    "    myargs = ''\n",
    "    myargs += f' --train_coords {TFRECORDFILE} '\n",
    "    myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "    myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "    myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "    myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "    myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "    myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "    myargs += f' --batch_size {BATCHSIZE} '\n",
    "    myargs += f' --optimizer {OPTIMIZER} '\n",
    "    myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "    myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "    add_job(name=f'train_mag{mag}',\n",
    "            workflow='ffn_training',\n",
    "            application='trainer',\n",
    "            args=myargs,\n",
    "            ranks_per_node=rpn,\n",
    "            num_nodes=nnode,\n",
    "            environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Space Bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a training job to the database\n",
    "import time\n",
    "TFRECORDFILE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/tf_record_file'\n",
    "GROUNDTRUTH='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/groundtruth.h5'\n",
    "GRAYSCALE='/lus/theta-fs0/projects/datascience/keceli/run/f3n/training/grayscale_maps.h5'\n",
    "BATCHSIZE=1\n",
    "OPTIMIZER='adam'\n",
    "TIMESTAMP=time.strftime(\"%y%m%d%H%M%S\")\n",
    "for rpn in [1,4,16]:\n",
    "    for nnode in [1,4,16,64]:\n",
    "        TRAINDIR=f'train_b{BATCHSIZE}_o{OPTIMIZER}_n{nnode}_r{rpn}_{TIMESTAMP}'\n",
    "        myargs = ''\n",
    "        myargs += f' --train_coords {TFRECORDFILE} '\n",
    "        myargs += f' --data_volumes valdation1:{GRAYSCALE}:raw '\n",
    "        myargs += f' --label_volumes valdation1:{GROUNDTRUTH}:stack '\n",
    "        myargs += f' --model_name convstack_3d.ConvStack3DFFNModel '\n",
    "        myargs += ''' --model_args \"{\\\\\"depth\\\\\": 12, \\\\\"fov_size\\\\\": [33, 33, 33], \\\\\"deltas\\\\\": [8, 8, 8]}\"'''\n",
    "        myargs += ' --image_mean 128 --image_stddev 33 '\n",
    "        myargs += ' --max_steps 40000000 --summary_rate_secs 360 ' \n",
    "        myargs += f' --batch_size {BATCHSIZE} '\n",
    "        myargs += f' --optimizer {OPTIMIZER} '\n",
    "        myargs += ' --num_intra_threads 64 --num_inter_threads 1 '\n",
    "        myargs += f' --train_dir {TRAINDIR} '\n",
    "\n",
    "        add_job(name=f'train_n{nnode}_r{rpn}',\n",
    "                workflow='ffn_training',\n",
    "                application='trainer',\n",
    "                args=myargs,\n",
    "                ranks_per_node=rpn,\n",
    "                num_nodes=nnode,\n",
    "                environ_vars={'OMP_NUM_THREADS=64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'happyneuron'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-35b523aa25b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m##ALIGNTK APPS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhappyneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maligntk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpreprocess_main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhappyneuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mglob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'happyneuron'"
     ]
    }
   ],
   "source": [
    "##ALIGNTK APPS \n",
    "\n",
    "from glob import glob\n",
    "\n",
    "##TODO jobalize the layers bellow\n",
    "\n",
    "#!/bin/bash\n",
    "#COBALT -t 180\n",
    "#COBALT -n 128\n",
    "#COBALT -q default\n",
    "#COBALT -A connectomics_aesp\n",
    "\n",
    "# NODES=$COBALT_JOBSIZE\n",
    "# PROC_PER_NODE=64\n",
    "# PROC=$((NODES * PROC_PER_NODE))\n",
    "PROC=777\n",
    "\n",
    "ALIGNTK_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/software/aligntk-1.0.2/install/bin\"\n",
    "IMAGE_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/trakem2_HL00732_run/trakem2_HL00732_process_1/output/\"\n",
    "MASK_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/aligntk_HL00732/out1_mask\"\n",
    "OUTPUT_DIR=\"/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/aligntk_HL00732/out1\"\n",
    "GROUP_SIZE=PROC-1\n",
    "N_IMAGES= len(glob(IMAGE_DIR+'*'))\n",
    "N_GROUPS=((N_IMAGES / GROUP_SIZE + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total image count: \"+str(N_IMAGES))\n",
    "print(\"Number of groups: \"+str(N_GROUPS))\n",
    "\n",
    "#aligntk_create_dirs(OUTPUT_DIR, MASK_DIR)\n",
    "#make_schedule(OUTPUT_DIR)\n",
    "#preprocess_main(IMAGE_DIR, OUTPUT_DIR, 12)\n",
    "\n",
    "from happyneuron.aligntk.preprocess import *\n",
    "\n",
    "def aligntk_preprocess_job(image_dir, mask_dir, low, high, kernel, workflow)\n",
    "\n",
    "\n",
    "  #aprun -n 777 -N $PROC_PER_NODE python -m klab_utils.aligntk_gen_mask --image_dir $IMAGE_DIR --mask_dir $MASK_DIR --low 10 --high 240 --kernel 10\n",
    "      pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "add_app(name='aligntk_findrst',\n",
    "        executable='python /lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/happyneuron/aligntk/find_rst.py',\n",
    "        description='AlignTK Apply Map',\n",
    "        envscript=env_preamble)\n",
    "\n",
    "def aligntk_rst_job( aligntk=''):\n",
    "\n",
    "    \n",
    "    for i in np.arange(N_GROUPS):\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/find_rst -pairs pairs$i.lst -tif -images $IMAGE_DIR -mask $MASK_DIR -output $OUTPUT_DIR/cmaps/ -rotation -15-15 -max_res 8192 -scale 0.8-1.2 -tx -30-30 -ty -30-30 -summary $OUTPUT_DIR/cmaps/summary$i.out\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_register_job():\n",
    "#for (( i=0; i<$N_GROUPS; i++ ))\n",
    "\n",
    "#  aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs$i.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 6.0 -output_level 6 -depth 6 -quality 0.5 -summary $OUTPUT_DIR/maps/summary$i.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "##\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/register -pairs pairs.lst -images $IMAGE_DIR -mask $MASK_DIR -tif -output $OUTPUT_DIR/maps/ -distortion 4.0 -output_level 6 -depth 6 -quality 0.3 -summary $OUTPUT_DIR/maps/summary.out -initial_map $OUTPUT_DIR/cmaps/\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_align_job():\n",
    "#FIXED=`head images.lst -n 1`\n",
    "# aprun -n 8192 -N 64 $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -maps $OUTPUT_DIR/maps/ -masks $MASK_DIR -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "#aprun -n $PROC -N $PROC_PER_NODE $ALIGNTK_DIR/align -images $IMAGE_DIR -image_list images.lst -map_list pairs.lst -masks $MASK_DIR -maps $OUTPUT_DIR/maps/ -output $OUTPUT_DIR/amaps/ -schedule schedule.lst -incremental -output_grid $OUTPUT_DIR/grids/ -grid_size 8192x8192 -fold_recovery 60\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligntk_applymap_job():\n",
    "##$ALIGNTK_DIR/apply_map -image_list images.lst -images $IMAGE_DIR -maps $OUTPUT_DIR/amaps/ -output $OUTPUT_DIR/aligned/ -memory 150000\n",
    "#aprun -n 777 -N 64 python -m klab_utils.aligntk_mpi_apply_map ./outputs_v1/ --image_dir ../data/images_corr_v2/ --image_lst ./images.lst\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhub_balsam",
   "language": "python",
   "name": "jhub_balsam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
