{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting an HDF5 Dataset to a Cloud Volume Image Layer\n",
    "\n",
    "This module demonstrates how to convert a monolothic 3D HDF5 dataset into a CloudVolume image layer. The dataset is assumed to be in a single .h5 file and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to add balsam and the modules it depends on to the Python search paths. \n",
    "import sys\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/env/lib/python3.6/site-packages/')\n",
    "sys.path.insert(0,'/soft/datascience/Balsam/0.3.5.1/')\n",
    "\n",
    "# We also need balsam and postgresql to be in the path. (Misha suggests this may not be necessary)\n",
    "import os\n",
    "os.environ['PATH'] ='/soft/datascience/Balsam/0.3.5.1/env/bin/:' + os.environ['PATH']\n",
    "os.environ['PATH'] +=':/soft/datascience/PostgreSQL/9.6.12/bin/'\n",
    "try:\n",
    "    import balsam\n",
    "except ImportError:\n",
    "    print('Cannot find balsam, make sure balsam is installed or it is available in Python search paths')\n",
    "    \n",
    "os.environ[\"BALSAM_DB_PATH\"]='/lus/theta-fs0/projects/connectomics_aesp/balsam_database/'\n",
    "\n",
    "from balsam_helper import *\n",
    "\n",
    "# Import widgets\n",
    "from ipywidgets import interact, interactive\n",
    "from ipywidgets import fixed, interact_manual \n",
    "from ipywidgets import Textarea, widgets, Layout, Accordion\n",
    "from ipywidgets import VBox, HBox, Box, Text, BoundedIntText\n",
    "\n",
    "env_preamble = '/lus/theta-fs0/projects/connectomics_aesp/software/HappyNeuron/macros_theta/theta_build_preamble.sh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Application\n",
    "\n",
    "Before submitting HDF5 conversion jobs, make sure there is a Balsam application available to you. Here, we set up a HappyNeuron application and h52cv workflow. The application will run the h52cv executable installed with HappyNeuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_app(\n",
    "    'HappyNeuron_h52cv',\n",
    "    'python /lus/theta-fs0/projects/software/HappyNeuron/happyneuron/io/hdf5_to_cloudvolume.py',  # 'hdf5_to_cloudvolume',\n",
    "    description='Convert images to a CloudVolume layer.',\n",
    "    envscript=env_preamble\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Workflow and Job\n",
    "\n",
    "This is where job parameters will be set and added to a workflow in the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/h52cv/rafcube.h5'\n",
    "cv_path = '/lus/theta-fs0/projects/connectomics_aesp/pipeline_data/h52cv/precomputed/image'\n",
    "key = 'image'\n",
    "\n",
    "args = f'--input {img_path} --output {cv_path} --key {key}'\n",
    "\n",
    "job_id = add_job(\n",
    "    'convert_rafcube',  # Job Name\n",
    "    'h52cv_rafcube',   # Workflow Name\n",
    "    'HappyNeuron_h52cv',\n",
    "    description='Convert the rafcube monolithic h5 to a CloudVolume image layer.',\n",
    "    args=args,\n",
    "    num_nodes=2,\n",
    "    ranks_per_node=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(project='connectomics_aesp',\n",
    "   queue='debug-cache-quad',\n",
    "   nodes=2,\n",
    "   wall_minutes=20,\n",
    "   wf_filter='h52cv_rafcube'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_info(job_id='',show_output=False):\n",
    "    \"\"\"\n",
    "    Prints verbose job info for a given job id.\n",
    "    Parameters\n",
    "    ----------\n",
    "    job_id: str, Partial or full Balsam job id.\n",
    "    \"\"\"\n",
    "    from balsam.launcher.dag import BalsamJob as Job\n",
    "    jobs = Job.objects.all().filter(job_id__contains=job_id)\n",
    "    if len(jobs) == 1:\n",
    "        thejob = jobs[0]\n",
    "        print(jobs[0])\n",
    "        if show_output:\n",
    "            output = f'{thejob.working_directory}/{thejob.name}.out'\n",
    "            with open(output) as f:\n",
    "                out = f.read()\n",
    "            print(f'Output file {output} content:')\n",
    "            print(out)\n",
    "    elif len(jobs) == 0:\n",
    "        print('No matching jobs')\n",
    "    else:\n",
    "        print(f'{len(jobs)} jobs matched, enter full id.')\n",
    "        \n",
    "get_job_info(job_id=job_id, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
